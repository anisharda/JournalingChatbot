Loading gmp version 6.2.1
Loading mpfr version 3.1.6
Loading mpc version 1.1.0
Loading zlib-ng version 2.1.6
Loading gcc version 9.4.0
2025-03-30 00:16:20.713952: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1743293781.309157 2155917 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1743293781.498223 2155917 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1743293783.000754 2155917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743293783.000805 2155917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743293783.000809 2155917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743293783.000812 2155917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-03-30 00:16:23.132775: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [04:19<08:38, 259.28s/it]Downloading shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [08:39<04:20, 260.01s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [11:17<00:00, 213.26s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [11:17<00:00, 225.81s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [01:24<02:49, 84.52s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [02:49<01:24, 84.65s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:42<00:00, 70.19s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:42<00:00, 74.08s/it]
Map:   0%|          | 0/76673 [00:00<?, ? examples/s]Map:   1%|â–         | 1000/76673 [00:00<00:34, 2200.38 examples/s]Map:   3%|â–Ž         | 2000/76673 [00:00<00:30, 2477.34 examples/s]Map:   4%|â–         | 3000/76673 [00:01<00:28, 2620.47 examples/s]Map:   5%|â–Œ         | 4000/76673 [00:01<00:27, 2673.19 examples/s]Map:   7%|â–‹         | 5000/76673 [00:01<00:29, 2471.22 examples/s]Map:   8%|â–Š         | 6000/76673 [00:02<00:27, 2576.29 examples/s]Map:   9%|â–‰         | 7000/76673 [00:02<00:28, 2420.52 examples/s]Map:  10%|â–ˆ         | 8000/76673 [00:03<00:27, 2466.81 examples/s]Map:  12%|â–ˆâ–        | 9000/76673 [00:03<00:26, 2547.59 examples/s]Map:  13%|â–ˆâ–Ž        | 10000/76673 [00:03<00:26, 2541.67 examples/s]Map:  14%|â–ˆâ–        | 11000/76673 [00:04<00:24, 2641.81 examples/s]Map:  16%|â–ˆâ–Œ        | 12000/76673 [00:04<00:30, 2142.97 examples/s]Map:  17%|â–ˆâ–‹        | 13000/76673 [00:05<00:29, 2189.96 examples/s]Map:  18%|â–ˆâ–Š        | 14000/76673 [00:05<00:26, 2343.04 examples/s]Map:  20%|â–ˆâ–‰        | 15000/76673 [00:06<00:25, 2460.57 examples/s]Map:  21%|â–ˆâ–ˆ        | 16000/76673 [00:06<00:27, 2210.34 examples/s]Map:  22%|â–ˆâ–ˆâ–       | 17000/76673 [00:07<00:25, 2308.36 examples/s]Map:  23%|â–ˆâ–ˆâ–Ž       | 18000/76673 [00:07<00:25, 2305.19 examples/s]Map:  25%|â–ˆâ–ˆâ–       | 19000/76673 [00:08<00:28, 2014.16 examples/s]Map:  26%|â–ˆâ–ˆâ–Œ       | 20000/76673 [00:08<00:25, 2206.08 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 21000/76673 [00:08<00:23, 2335.20 examples/s]Map:  29%|â–ˆâ–ˆâ–Š       | 22000/76673 [00:09<00:23, 2340.58 examples/s]Map:  30%|â–ˆâ–ˆâ–‰       | 23000/76673 [00:09<00:23, 2329.47 examples/s]Map:  31%|â–ˆâ–ˆâ–ˆâ–      | 24000/76673 [00:10<00:21, 2438.00 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 25000/76673 [00:10<00:20, 2531.70 examples/s]Map:  34%|â–ˆâ–ˆâ–ˆâ–      | 26000/76673 [00:10<00:20, 2458.58 examples/s]Map:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 27000/76673 [00:11<00:19, 2499.00 examples/s]Map:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 28000/76673 [00:11<00:18, 2575.08 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 29000/76673 [00:12<00:18, 2585.21 examples/s]Map:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 30000/76673 [00:12<00:17, 2623.35 examples/s]Map:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 31000/76673 [00:12<00:17, 2666.44 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 32000/76673 [00:13<00:17, 2548.93 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 33000/76673 [00:13<00:17, 2541.29 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 34000/76673 [00:14<00:17, 2454.15 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 35000/76673 [00:14<00:16, 2483.36 examples/s]Map:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 36000/76673 [00:14<00:15, 2575.33 examples/s]Map:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 37000/76673 [00:15<00:15, 2513.36 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 38000/76673 [00:15<00:16, 2377.85 examples/s]Map:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 39000/76673 [00:16<00:15, 2481.41 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 40000/76673 [00:16<00:17, 2075.26 examples/s]Map:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 41000/76673 [00:17<00:15, 2259.86 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 42000/76673 [00:17<00:14, 2391.45 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 43000/76673 [00:17<00:13, 2483.68 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 44000/76673 [00:18<00:12, 2577.22 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 45000/76673 [00:18<00:12, 2633.18 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 46000/76673 [00:18<00:11, 2702.67 examples/s]Map:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 47000/76673 [00:19<00:10, 2747.61 examples/s]Map:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 48000/76673 [00:19<00:10, 2757.74 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 49000/76673 [00:19<00:10, 2618.22 examples/s]Map:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 50000/76673 [00:20<00:11, 2408.47 examples/s]Map:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 51000/76673 [00:20<00:10, 2428.81 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 52000/76673 [00:21<00:10, 2363.74 examples/s]Map:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 53000/76673 [00:21<00:09, 2476.48 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 54000/76673 [00:22<00:08, 2560.38 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 55000/76673 [00:22<00:08, 2467.29 examples/s]Map:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 56000/76673 [00:22<00:08, 2574.54 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 57000/76673 [00:23<00:08, 2354.08 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 58000/76673 [00:23<00:07, 2445.46 examples/s]Map:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 59000/76673 [00:24<00:06, 2554.62 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 60000/76673 [00:24<00:06, 2626.96 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 61000/76673 [00:25<00:08, 1883.88 examples/s]Map:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 62000/76673 [00:25<00:07, 1985.59 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 63000/76673 [00:26<00:06, 2154.94 examples/s]Map:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 64000/76673 [00:26<00:05, 2112.66 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 65000/76673 [00:27<00:05, 2166.19 examples/s]Map:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 66000/76673 [00:27<00:04, 2238.72 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 67000/76673 [00:27<00:04, 2385.23 examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 68000/76673 [00:28<00:03, 2378.53 examples/s]Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 69000/76673 [00:28<00:03, 2387.55 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 70000/76673 [00:28<00:02, 2501.32 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 71000/76673 [00:29<00:02, 2581.70 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 72000/76673 [00:29<00:02, 2272.37 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 73000/76673 [00:30<00:01, 2372.90 examples/s]Map:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 74000/76673 [00:30<00:01, 2353.45 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 75000/76673 [00:31<00:00, 2434.20 examples/s]Map:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 76000/76673 [00:31<00:00, 2539.79 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76673/76673 [00:31<00:00, 2548.48 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76673/76673 [00:36<00:00, 2118.36 examples/s]
Map:   0%|          | 0/12030 [00:00<?, ? examples/s]Map:   8%|â–Š         | 1000/12030 [00:00<00:05, 2190.75 examples/s]Map:  17%|â–ˆâ–‹        | 2000/12030 [00:00<00:04, 2431.43 examples/s]Map:  25%|â–ˆâ–ˆâ–       | 3000/12030 [00:01<00:03, 2363.55 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4000/12030 [00:01<00:03, 2509.33 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5000/12030 [00:02<00:02, 2476.47 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 6000/12030 [00:02<00:02, 2042.22 examples/s]Map:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7000/12030 [00:03<00:02, 2235.34 examples/s]Map:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8000/12030 [00:03<00:01, 2396.20 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 9000/12030 [00:03<00:01, 2483.82 examples/s]Map:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10000/12030 [00:04<00:00, 2463.86 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11000/12030 [00:04<00:00, 2456.33 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 12000/12030 [00:05<00:00, 2378.52 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12030/12030 [00:05<00:00, 2035.74 examples/s]
Map:   0%|          | 0/10943 [00:00<?, ? examples/s]Map:   9%|â–‰         | 1000/10943 [00:00<00:04, 2216.63 examples/s]Map:  18%|â–ˆâ–Š        | 2000/10943 [00:00<00:03, 2257.24 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 3000/10943 [00:01<00:03, 2254.18 examples/s]Map:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 4000/10943 [00:01<00:02, 2425.76 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 5000/10943 [00:02<00:02, 2553.09 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 6000/10943 [00:02<00:01, 2622.11 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 7000/10943 [00:02<00:01, 2670.17 examples/s]Map:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 8000/10943 [00:03<00:01, 2367.08 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 9000/10943 [00:03<00:00, 2335.53 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 10000/10943 [00:04<00:00, 2401.68 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10943/10943 [00:04<00:00, 2510.18 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10943/10943 [00:05<00:00, 2101.14 examples/s]
/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/work/pi_jaimedavila_umass_edu/asharda_umass_edu/LLM_scripts/llama2_7b/train_bot.py:98: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/28752 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
  0%|          | 1/28752 [02:13<1067:11:30, 133.63s/it]  0%|          | 2/28752 [04:26<1063:29:34, 133.17s/it]  0%|          | 3/28752 [06:38<1060:37:34, 132.81s/it]  0%|          | 4/28752 [08:51<1059:14:22, 132.64s/it]  0%|          | 5/28752 [11:03<1059:04:31, 132.63s/it]  0%|          | 6/28752 [13:16<1058:38:51, 132.58s/it]  0%|          | 7/28752 [15:28<1058:04:23, 132.51s/it]  0%|          | 8/28752 [17:41<1059:02:31, 132.64s/it]  0%|          | 9/28752 [19:54<1059:03:18, 132.64s/it]  0%|          | 10/28752 [22:06<1058:52:31, 132.63s/it]                                                          0%|          | 10/28752 [22:06<1058:52:31, 132.63s/it]  0%|          | 11/28752 [24:19<1059:10:21, 132.67s/it]  0%|          | 12/28752 [26:32<1059:13:54, 132.68s/it]  0%|          | 13/28752 [28:45<1059:17:16, 132.69s/it]  0%|          | 14/28752 [30:57<1058:58:44, 132.66s/it]  0%|          | 15/28752 [33:10<1059:07:33, 132.68s/it]  0%|          | 16/28752 [35:22<1058:56:54, 132.66s/it]  0%|          | 17/28752 [37:35<1058:44:43, 132.64s/it]  0%|          | 18/28752 [39:48<1058:30:18, 132.62s/it]  0%|          | 19/28752 [42:00<1058:14:36, 132.59s/it]slurmstepd-gypsum-gpu073: error: *** JOB 30777712 ON gypsum-gpu073 CANCELLED AT 2025-03-30T01:16:07 DUE TO TIME LIMIT ***
