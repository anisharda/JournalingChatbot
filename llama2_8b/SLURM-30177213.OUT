Loading gmp version 6.2.1
Loading mpfr version 3.1.6
Loading mpc version 1.1.0
Loading zlib-ng version 2.1.6
Loading gcc version 9.4.0
/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2025-03-24 14:34:40.490663: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742826881.100605 3551632 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742826881.336043 3551632 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1742826883.065895 3551632 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1742826883.066013 3551632 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1742826883.066022 3551632 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1742826883.066028 3551632 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-03-24 14:34:43.229708: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Downloading data:   0%|          | 0.00/28.0M [00:00<?, ?B/s]Downloading data:  21%|â–ˆâ–ˆ        | 5.94M/28.0M [00:00<00:00, 56.4MB/s]Downloading data:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 12.6M/28.0M [00:00<00:00, 62.4MB/s]Downloading data:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 23.4M/28.0M [00:00<00:00, 82.6MB/s]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.0M/28.0M [00:00<00:00, 47.7MB/s]
Generating train split:   0%|          | 0/76673 [00:00<?, ? examples/s]Generating train split:   0%|          | 1/76673 [00:00<7:34:37,  2.81 examples/s]Generating train split:   1%|â–         | 1000/76673 [00:00<00:29, 2580.19 examples/s]Generating train split:   3%|â–Ž         | 2225/76673 [00:00<00:14, 5185.01 examples/s]Generating train split:   5%|â–         | 3790/76673 [00:00<00:09, 7995.40 examples/s]Generating train split:   7%|â–‹         | 5161/76673 [00:00<00:07, 9604.78 examples/s]Generating train split:   9%|â–Š         | 6669/76673 [00:00<00:06, 11174.69 examples/s]Generating train split:  11%|â–ˆ         | 8204/76673 [00:00<00:05, 12386.09 examples/s]Generating train split:  13%|â–ˆâ–Ž        | 10068/76673 [00:01<00:04, 14219.11 examples/s]Generating train split:  16%|â–ˆâ–Œ        | 12000/76673 [00:01<00:04, 15534.07 examples/s]Generating train split:  18%|â–ˆâ–Š        | 14000/76673 [00:01<00:03, 16506.11 examples/s]Generating train split:  21%|â–ˆâ–ˆ        | 15845/76673 [00:01<00:03, 17071.21 examples/s]Generating train split:  23%|â–ˆâ–ˆâ–Ž       | 17644/76673 [00:01<00:03, 17332.56 examples/s]Generating train split:  25%|â–ˆâ–ˆâ–Œ       | 19476/76673 [00:01<00:03, 17623.65 examples/s]Generating train split:  28%|â–ˆâ–ˆâ–Š       | 21314/76673 [00:01<00:03, 17846.65 examples/s]Generating train split:  30%|â–ˆâ–ˆâ–ˆ       | 23138/76673 [00:01<00:02, 17962.06 examples/s]Generating train split:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 25000/76673 [00:01<00:02, 17987.96 examples/s]Generating train split:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 27000/76673 [00:02<00:02, 18139.87 examples/s]Generating train split:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 29000/76673 [00:02<00:02, 18224.26 examples/s]Generating train split:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 31000/76673 [00:02<00:02, 18216.36 examples/s]Generating train split:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 33000/76673 [00:02<00:02, 18345.93 examples/s]Generating train split:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 34998/76673 [00:02<00:02, 18521.36 examples/s]Generating train split:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 37738/76673 [00:02<00:02, 18423.01 examples/s]Generating train split:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 40218/76673 [00:02<00:02, 17759.67 examples/s]Generating train split:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 42074/76673 [00:02<00:01, 17955.83 examples/s]Generating train split:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 44000/76673 [00:02<00:01, 18081.33 examples/s]Generating train split:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 46000/76673 [00:03<00:01, 18212.57 examples/s]Generating train split:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 48000/76673 [00:03<00:01, 18278.73 examples/s]Generating train split:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 50000/76673 [00:03<00:01, 18333.63 examples/s]Generating train split:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 52000/76673 [00:03<00:01, 18337.88 examples/s]Generating train split:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 54000/76673 [00:03<00:01, 18423.16 examples/s]Generating train split:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 56000/76673 [00:03<00:01, 18415.02 examples/s]Generating train split:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 57908/76673 [00:03<00:01, 18599.13 examples/s]Generating train split:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 60611/76673 [00:03<00:00, 18375.53 examples/s]Generating train split:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 63292/76673 [00:04<00:00, 18196.05 examples/s]Generating train split:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 66000/76673 [00:04<00:00, 18091.96 examples/s]Generating train split:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 68000/76673 [00:04<00:00, 18175.51 examples/s]Generating train split:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 70000/76673 [00:04<00:00, 18259.51 examples/s]Generating train split:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 71975/76673 [00:04<00:00, 18368.72 examples/s]Generating train split:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 74692/76673 [00:04<00:00, 18276.49 examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76673/76673 [00:04<00:00, 18276.63 examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76673/76673 [00:04<00:00, 15638.10 examples/s]
Generating validation split:   0%|          | 0/12030 [00:00<?, ? examples/s]Generating validation split:   0%|          | 1/12030 [00:00<41:47,  4.80 examples/s]Generating validation split:   8%|â–Š         | 1000/12030 [00:00<00:02, 3977.29 examples/s]Generating validation split:  17%|â–ˆâ–‹        | 2101/12030 [00:00<00:01, 6524.83 examples/s]Generating validation split:  25%|â–ˆâ–ˆâ–Œ       | 3057/12030 [00:00<00:01, 7556.08 examples/s]Generating validation split:  35%|â–ˆâ–ˆâ–ˆâ–      | 4161/12030 [00:00<00:00, 8694.69 examples/s]Generating validation split:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 5646/12030 [00:00<00:00, 9187.29 examples/s]Generating validation split:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 7101/12030 [00:00<00:00, 9371.25 examples/s]Generating validation split:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 8185/12030 [00:01<00:00, 9753.82 examples/s]Generating validation split:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 9684/12030 [00:01<00:00, 9835.78 examples/s]Generating validation split:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 11145/12030 [00:01<00:00, 9798.96 examples/s]Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12030/12030 [00:01<00:00, 8414.64 examples/s]
Generating test split:   0%|          | 0/10943 [00:00<?, ? examples/s]Generating test split:  10%|â–‰         | 1070/10943 [00:00<00:00, 10371.41 examples/s]Generating test split:  20%|â–ˆâ–‰        | 2171/10943 [00:00<00:00, 10736.27 examples/s]Generating test split:  30%|â–ˆâ–ˆâ–‰       | 3269/10943 [00:00<00:00, 10841.97 examples/s]Generating test split:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 4356/10943 [00:00<00:00, 10852.49 examples/s]Generating test split:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 5449/10943 [00:00<00:00, 10876.43 examples/s]Generating test split:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6579/10943 [00:00<00:00, 10920.36 examples/s]Generating test split:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7676/10943 [00:00<00:00, 10933.73 examples/s]Generating test split:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 9259/10943 [00:00<00:00, 10772.99 examples/s]Generating test split:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 10372/10943 [00:00<00:00, 10866.50 examples/s]Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10943/10943 [00:01<00:00, 10524.41 examples/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [03:34<07:08, 214.12s/it]Downloading shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [06:55<03:26, 206.63s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [09:02<00:00, 170.21s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [09:02<00:00, 180.79s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [01:27<02:55, 87.78s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [02:56<01:28, 88.27s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:51<00:00, 72.98s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:51<00:00, 77.06s/it]
Map:   0%|          | 0/76673 [00:00<?, ? examples/s]Map:   1%|â–         | 1000/76673 [00:00<00:22, 3329.76 examples/s]Map:   3%|â–Ž         | 2000/76673 [00:00<00:23, 3226.58 examples/s]Map:   4%|â–         | 3000/76673 [00:00<00:22, 3273.72 examples/s]Map:   5%|â–Œ         | 4000/76673 [00:01<00:22, 3234.18 examples/s]Map:   7%|â–‹         | 5000/76673 [00:01<00:25, 2808.96 examples/s]Map:   8%|â–Š         | 6000/76673 [00:01<00:23, 2984.00 examples/s]Map:   9%|â–‰         | 7000/76673 [00:02<00:27, 2564.83 examples/s]Map:  10%|â–ˆ         | 8000/76673 [00:02<00:26, 2634.45 examples/s]Map:  12%|â–ˆâ–        | 9000/76673 [00:03<00:23, 2846.68 examples/s]Map:  13%|â–ˆâ–Ž        | 10000/76673 [00:03<00:23, 2878.33 examples/s]Map:  14%|â–ˆâ–        | 11000/76673 [00:03<00:22, 2880.64 examples/s]Map:  16%|â–ˆâ–Œ        | 12000/76673 [00:04<00:22, 2840.58 examples/s]Map:  17%|â–ˆâ–‹        | 13000/76673 [00:04<00:21, 2934.60 examples/s]Map:  18%|â–ˆâ–Š        | 14000/76673 [00:05<00:26, 2344.27 examples/s]Map:  20%|â–ˆâ–‰        | 15000/76673 [00:05<00:24, 2568.72 examples/s]Map:  21%|â–ˆâ–ˆ        | 16000/76673 [00:05<00:25, 2341.73 examples/s]Map:  22%|â–ˆâ–ˆâ–       | 17000/76673 [00:06<00:24, 2449.67 examples/s]Map:  23%|â–ˆâ–ˆâ–Ž       | 18000/76673 [00:06<00:22, 2553.76 examples/s]Map:  25%|â–ˆâ–ˆâ–       | 19000/76673 [00:06<00:21, 2667.19 examples/s]Map:  26%|â–ˆâ–ˆâ–Œ       | 20000/76673 [00:07<00:19, 2834.03 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 21000/76673 [00:07<00:19, 2821.95 examples/s]Map:  29%|â–ˆâ–ˆâ–Š       | 22000/76673 [00:08<00:20, 2672.28 examples/s]Map:  30%|â–ˆâ–ˆâ–‰       | 23000/76673 [00:08<00:18, 2844.59 examples/s]Map:  31%|â–ˆâ–ˆâ–ˆâ–      | 24000/76673 [00:08<00:17, 2990.55 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 25000/76673 [00:08<00:16, 3053.49 examples/s]Map:  34%|â–ˆâ–ˆâ–ˆâ–      | 26000/76673 [00:09<00:21, 2369.95 examples/s]Map:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 27000/76673 [00:09<00:19, 2606.23 examples/s]Map:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 28000/76673 [00:10<00:17, 2754.22 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 29000/76673 [00:10<00:17, 2793.85 examples/s]Map:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 30000/76673 [00:10<00:16, 2903.75 examples/s]Map:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 31000/76673 [00:11<00:15, 2978.44 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 32000/76673 [00:11<00:15, 2925.78 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 33000/76673 [00:11<00:14, 3043.72 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 34000/76673 [00:12<00:14, 2933.93 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 35000/76673 [00:12<00:14, 2828.21 examples/s]Map:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 36000/76673 [00:12<00:13, 2965.31 examples/s]Map:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 37000/76673 [00:13<00:12, 3075.27 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 38000/76673 [00:13<00:13, 2861.14 examples/s]Map:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 39000/76673 [00:13<00:13, 2857.62 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 40000/76673 [00:14<00:12, 2967.42 examples/s]Map:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 41000/76673 [00:14<00:11, 3082.81 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 42000/76673 [00:14<00:10, 3170.24 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 43000/76673 [00:15<00:10, 3145.92 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 44000/76673 [00:15<00:10, 3176.62 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 45000/76673 [00:15<00:09, 3234.04 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 46000/76673 [00:16<00:09, 3258.60 examples/s]Map:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 47000/76673 [00:16<00:09, 3260.91 examples/s]Map:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 48000/76673 [00:16<00:09, 3146.17 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 49000/76673 [00:17<00:09, 3035.08 examples/s]Map:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 50000/76673 [00:17<00:09, 2864.16 examples/s]Map:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 51000/76673 [00:17<00:08, 2869.43 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 52000/76673 [00:18<00:08, 2915.74 examples/s]Map:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 53000/76673 [00:18<00:07, 3004.47 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 54000/76673 [00:18<00:07, 3096.27 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 55000/76673 [00:19<00:07, 2852.38 examples/s]Map:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 56000/76673 [00:19<00:08, 2357.50 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 57000/76673 [00:20<00:08, 2311.00 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 58000/76673 [00:20<00:07, 2494.42 examples/s]Map:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 59000/76673 [00:20<00:06, 2715.14 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 60000/76673 [00:21<00:05, 2917.11 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 61000/76673 [00:21<00:06, 2464.45 examples/s]Map:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 62000/76673 [00:22<00:05, 2607.73 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 63000/76673 [00:22<00:04, 2810.99 examples/s]Map:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 64000/76673 [00:22<00:04, 2635.05 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 65000/76673 [00:23<00:04, 2560.08 examples/s]Map:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 66000/76673 [00:23<00:04, 2617.59 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 67000/76673 [00:23<00:03, 2794.37 examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 68000/76673 [00:24<00:02, 2921.70 examples/s]Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 69000/76673 [00:24<00:02, 3028.66 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 70000/76673 [00:24<00:02, 3087.17 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 71000/76673 [00:25<00:01, 3128.68 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 72000/76673 [00:25<00:01, 2663.11 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 73000/76673 [00:25<00:01, 2642.07 examples/s]Map:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 74000/76673 [00:26<00:01, 2660.99 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 75000/76673 [00:26<00:00, 2653.17 examples/s]Map:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 76000/76673 [00:27<00:00, 2628.48 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76673/76673 [00:27<00:00, 2619.65 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76673/76673 [00:28<00:00, 2682.21 examples/s]
Map:   0%|          | 0/12030 [00:00<?, ? examples/s]Map:   8%|â–Š         | 1000/12030 [00:00<00:03, 2776.89 examples/s]Map:  17%|â–ˆâ–‹        | 2000/12030 [00:00<00:03, 2987.62 examples/s]Map:  25%|â–ˆâ–ˆâ–       | 3000/12030 [00:00<00:02, 3212.37 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4000/12030 [00:01<00:02, 3311.49 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5000/12030 [00:01<00:02, 2972.48 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 6000/12030 [00:01<00:01, 3068.62 examples/s]Map:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7000/12030 [00:02<00:02, 2401.74 examples/s]Map:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8000/12030 [00:02<00:01, 2618.02 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 9000/12030 [00:03<00:01, 2768.35 examples/s]Map:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10000/12030 [00:03<00:00, 2742.57 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11000/12030 [00:03<00:00, 2767.55 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 12000/12030 [00:04<00:00, 2898.59 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12030/12030 [00:04<00:00, 2537.60 examples/s]
Map:   0%|          | 0/10943 [00:00<?, ? examples/s]Map:   9%|â–‰         | 1000/10943 [00:00<00:03, 2691.07 examples/s]Map:  18%|â–ˆâ–Š        | 2000/10943 [00:00<00:03, 2781.22 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 3000/10943 [00:01<00:02, 2877.02 examples/s]Map:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 4000/10943 [00:01<00:02, 3020.60 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 5000/10943 [00:01<00:01, 3088.93 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 6000/10943 [00:01<00:01, 3157.36 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 7000/10943 [00:02<00:01, 3147.77 examples/s]Map:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 8000/10943 [00:02<00:01, 2790.45 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 9000/10943 [00:03<00:00, 2868.95 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 10000/10943 [00:03<00:00, 2970.50 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10943/10943 [00:03<00:00, 2918.16 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10943/10943 [00:04<00:00, 2615.91 examples/s]
/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/work/pi_jaimedavila_umass_edu/asharda_umass_edu/LLM_scripts/llama2_7b/train_bot.py:165: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/28752 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
Traceback (most recent call last):
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/LLM_scripts/llama2_7b/train_bot.py", line 185, in <module>
    trainer.train()
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/trainer.py", line 3698, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/trainer.py", line 3759, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/peft/peft_model.py", line 1756, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/peft/tuners/tuners_utils.py", line 193, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/accelerate/hooks.py", line 176, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 863, in forward
    loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/loss/loss_utils.py", line 47, in ForCausalLMLoss
    loss = fixed_cross_entropy(logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/loss/loss_utils.py", line 26, in fixed_cross_entropy
    loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/torch/nn/functional.py", line 3494, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: Expected input batch_size (512) to match target batch_size (128).
  0%|          | 0/28752 [00:07<?, ?it/s]
