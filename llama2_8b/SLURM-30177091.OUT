Loading gmp version 6.2.1
Loading mpfr version 3.1.6
Loading mpc version 1.1.0
Loading zlib-ng version 2.1.6
Loading gcc version 9.4.0
2025-03-24 14:03:44.626601: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742825025.203169 1134415 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742825025.465081 1134415 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1742825026.923645 1134415 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1742825026.923693 1134415 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1742825026.923698 1134415 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1742825026.923701 1134415 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-03-24 14:03:47.061891: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [01:28<02:57, 88.60s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:58<01:29, 89.29s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:53<00:00, 73.66s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:53<00:00, 77.81s/it]
Map:   0%|          | 0/76673 [00:00<?, ? examples/s]Map:   1%|▏         | 1000/76673 [00:00<00:30, 2465.49 examples/s]Map:   3%|▎         | 2000/76673 [00:00<00:27, 2748.79 examples/s]Map:   4%|▍         | 3000/76673 [00:01<00:25, 2869.07 examples/s]Map:   5%|▌         | 4000/76673 [00:01<00:24, 2937.06 examples/s]Map:   7%|▋         | 5000/76673 [00:01<00:27, 2627.00 examples/s]Map:   8%|▊         | 6000/76673 [00:02<00:25, 2794.43 examples/s]Map:   9%|▉         | 7000/76673 [00:02<00:27, 2560.34 examples/s]Map:  10%|█         | 8000/76673 [00:02<00:25, 2651.50 examples/s]Map:  12%|█▏        | 9000/76673 [00:03<00:24, 2785.31 examples/s]Map:  13%|█▎        | 10000/76673 [00:03<00:23, 2793.18 examples/s]Map:  14%|█▍        | 11000/76673 [00:03<00:22, 2934.75 examples/s]Map:  16%|█▌        | 12000/76673 [00:04<00:22, 2881.59 examples/s]Map:  17%|█▋        | 13000/76673 [00:04<00:22, 2838.46 examples/s]Map:  18%|█▊        | 14000/76673 [00:05<00:22, 2847.00 examples/s]Map:  20%|█▉        | 15000/76673 [00:05<00:21, 2926.79 examples/s]Map:  21%|██        | 16000/76673 [00:05<00:23, 2559.18 examples/s]Map:  22%|██▏       | 17000/76673 [00:06<00:25, 2341.30 examples/s]Map:  23%|██▎       | 18000/76673 [00:06<00:23, 2467.04 examples/s]Map:  25%|██▍       | 19000/76673 [00:07<00:21, 2646.72 examples/s]Map:  26%|██▌       | 20000/76673 [00:07<00:20, 2759.69 examples/s]Map:  27%|██▋       | 21000/76673 [00:07<00:19, 2813.31 examples/s]Map:  29%|██▊       | 22000/76673 [00:08<00:25, 2120.50 examples/s]Map:  30%|██▉       | 23000/76673 [00:08<00:23, 2283.13 examples/s]Map:  31%|███▏      | 24000/76673 [00:09<00:21, 2431.95 examples/s]Map:  33%|███▎      | 25000/76673 [00:09<00:20, 2574.46 examples/s]Map:  34%|███▍      | 26000/76673 [00:09<00:18, 2677.93 examples/s]Map:  35%|███▌      | 27000/76673 [00:10<00:18, 2715.97 examples/s]Map:  37%|███▋      | 28000/76673 [00:10<00:17, 2755.01 examples/s]Map:  38%|███▊      | 29000/76673 [00:10<00:17, 2793.09 examples/s]Map:  39%|███▉      | 30000/76673 [00:11<00:16, 2797.08 examples/s]Map:  40%|████      | 31000/76673 [00:11<00:16, 2836.61 examples/s]Map:  42%|████▏     | 32000/76673 [00:11<00:15, 2839.87 examples/s]Map:  43%|████▎     | 33000/76673 [00:12<00:15, 2858.89 examples/s]Map:  44%|████▍     | 34000/76673 [00:12<00:15, 2799.79 examples/s]Map:  46%|████▌     | 35000/76673 [00:13<00:15, 2720.96 examples/s]Map:  47%|████▋     | 36000/76673 [00:13<00:14, 2762.59 examples/s]Map:  48%|████▊     | 37000/76673 [00:13<00:14, 2778.73 examples/s]Map:  50%|████▉     | 38000/76673 [00:14<00:14, 2696.49 examples/s]Map:  51%|█████     | 39000/76673 [00:14<00:13, 2827.13 examples/s]Map:  52%|█████▏    | 40000/76673 [00:14<00:12, 2944.39 examples/s]Map:  53%|█████▎    | 41000/76673 [00:15<00:12, 2950.94 examples/s]Map:  55%|█████▍    | 42000/76673 [00:15<00:11, 2929.13 examples/s]Map:  56%|█████▌    | 43000/76673 [00:15<00:11, 2887.19 examples/s]Map:  57%|█████▋    | 44000/76673 [00:16<00:10, 3005.04 examples/s]Map:  59%|█████▊    | 45000/76673 [00:16<00:12, 2452.25 examples/s]Map:  60%|█████▉    | 46000/76673 [00:16<00:11, 2658.92 examples/s]Map:  61%|██████▏   | 47000/76673 [00:17<00:10, 2832.74 examples/s]Map:  63%|██████▎   | 48000/76673 [00:17<00:09, 2948.57 examples/s]Map:  64%|██████▍   | 49000/76673 [00:17<00:09, 3047.32 examples/s]Map:  65%|██████▌   | 50000/76673 [00:18<00:09, 2700.58 examples/s]Map:  67%|██████▋   | 51000/76673 [00:18<00:09, 2654.20 examples/s]Map:  68%|██████▊   | 52000/76673 [00:19<00:08, 2777.34 examples/s]Map:  69%|██████▉   | 53000/76673 [00:19<00:08, 2929.62 examples/s]Map:  70%|███████   | 54000/76673 [00:19<00:07, 3018.40 examples/s]Map:  72%|███████▏  | 55000/76673 [00:20<00:07, 2815.13 examples/s]Map:  73%|███████▎  | 56000/76673 [00:20<00:07, 2850.02 examples/s]Map:  74%|███████▍  | 57000/76673 [00:20<00:07, 2586.82 examples/s]Map:  76%|███████▌  | 58000/76673 [00:21<00:07, 2660.71 examples/s]Map:  77%|███████▋  | 59000/76673 [00:21<00:06, 2800.64 examples/s]Map:  78%|███████▊  | 60000/76673 [00:21<00:05, 2813.09 examples/s]Map:  80%|███████▉  | 61000/76673 [00:22<00:06, 2317.61 examples/s]Map:  81%|████████  | 62000/76673 [00:22<00:05, 2545.81 examples/s]Map:  82%|████████▏ | 63000/76673 [00:23<00:04, 2736.92 examples/s]Map:  83%|████████▎ | 64000/76673 [00:23<00:05, 2524.63 examples/s]Map:  85%|████████▍ | 65000/76673 [00:24<00:04, 2433.60 examples/s]Map:  86%|████████▌ | 66000/76673 [00:24<00:04, 2559.27 examples/s]Map:  87%|████████▋ | 67000/76673 [00:24<00:04, 2213.70 examples/s]Map:  89%|████████▊ | 68000/76673 [00:25<00:03, 2440.69 examples/s]Map:  90%|████████▉ | 69000/76673 [00:25<00:02, 2599.69 examples/s]Map:  91%|█████████▏| 70000/76673 [00:25<00:02, 2659.96 examples/s]Map:  93%|█████████▎| 71000/76673 [00:26<00:02, 2728.42 examples/s]Map:  94%|█████████▍| 72000/76673 [00:26<00:01, 2442.74 examples/s]Map:  95%|█████████▌| 73000/76673 [00:27<00:01, 2542.51 examples/s]Map:  97%|█████████▋| 74000/76673 [00:27<00:00, 2737.86 examples/s]Map:  98%|█████████▊| 75000/76673 [00:27<00:00, 2863.57 examples/s]Map:  99%|█████████▉| 76000/76673 [00:28<00:00, 2978.34 examples/s]Map: 100%|██████████| 76673/76673 [00:28<00:00, 2994.64 examples/s]Map: 100%|██████████| 76673/76673 [00:30<00:00, 2475.44 examples/s]
Map:   0%|          | 0/12030 [00:00<?, ? examples/s]Map:   8%|▊         | 1000/12030 [00:00<00:03, 2843.05 examples/s]Map:  17%|█▋        | 2000/12030 [00:00<00:03, 3011.98 examples/s]Map:  25%|██▍       | 3000/12030 [00:00<00:02, 3127.37 examples/s]Map:  33%|███▎      | 4000/12030 [00:01<00:02, 3204.20 examples/s]Map:  42%|████▏     | 5000/12030 [00:01<00:02, 2823.80 examples/s]Map:  50%|████▉     | 6000/12030 [00:02<00:02, 2824.29 examples/s]Map:  58%|█████▊    | 7000/12030 [00:02<00:01, 2963.21 examples/s]Map:  67%|██████▋   | 8000/12030 [00:02<00:01, 3091.38 examples/s]Map:  75%|███████▍  | 9000/12030 [00:02<00:00, 3082.93 examples/s]Map:  83%|████████▎ | 10000/12030 [00:03<00:00, 2961.12 examples/s]Map:  91%|█████████▏| 11000/12030 [00:03<00:00, 2898.80 examples/s]Map: 100%|█████████▉| 12000/12030 [00:04<00:00, 2998.67 examples/s]Map: 100%|██████████| 12030/12030 [00:04<00:00, 2645.25 examples/s]
Map:   0%|          | 0/10943 [00:00<?, ? examples/s]Map:   9%|▉         | 1000/10943 [00:00<00:03, 2769.65 examples/s]Map:  18%|█▊        | 2000/10943 [00:00<00:03, 2834.71 examples/s]Map:  27%|██▋       | 3000/10943 [00:01<00:02, 2856.77 examples/s]Map:  37%|███▋      | 4000/10943 [00:01<00:02, 3007.99 examples/s]Map:  46%|████▌     | 5000/10943 [00:01<00:01, 3133.70 examples/s]Map:  55%|█████▍    | 6000/10943 [00:02<00:02, 2433.17 examples/s]Map:  64%|██████▍   | 7000/10943 [00:02<00:01, 2625.13 examples/s]Map:  73%|███████▎  | 8000/10943 [00:03<00:01, 2490.08 examples/s]Map:  82%|████████▏ | 9000/10943 [00:03<00:00, 2687.94 examples/s]Map:  91%|█████████▏| 10000/10943 [00:03<00:00, 2845.92 examples/s]Map: 100%|██████████| 10943/10943 [00:03<00:00, 2971.45 examples/s]Map: 100%|██████████| 10943/10943 [00:04<00:00, 2499.92 examples/s]
/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/work/pi_jaimedavila_umass_edu/asharda_umass_edu/LLM_scripts/llama2_7b/train_bot.py:135: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Traceback (most recent call last):
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/LLM_scripts/llama2_7b/train_bot.py", line 135, in <module>
    trainer = Trainer(
              ^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/trainer.py", line 558, in __init__
    raise ValueError(
ValueError: You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details
