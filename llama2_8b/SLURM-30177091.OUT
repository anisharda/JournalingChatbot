Loading gmp version 6.2.1
Loading mpfr version 3.1.6
Loading mpc version 1.1.0
Loading zlib-ng version 2.1.6
Loading gcc version 9.4.0
2025-03-24 14:03:44.626601: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742825025.203169 1134415 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742825025.465081 1134415 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1742825026.923645 1134415 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1742825026.923693 1134415 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1742825026.923698 1134415 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1742825026.923701 1134415 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-03-24 14:03:47.061891: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [01:28<02:57, 88.60s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [02:58<01:29, 89.29s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:53<00:00, 73.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:53<00:00, 77.81s/it]
Map:   0%|          | 0/76673 [00:00<?, ? examples/s]Map:   1%|â–         | 1000/76673 [00:00<00:30, 2465.49 examples/s]Map:   3%|â–Ž         | 2000/76673 [00:00<00:27, 2748.79 examples/s]Map:   4%|â–         | 3000/76673 [00:01<00:25, 2869.07 examples/s]Map:   5%|â–Œ         | 4000/76673 [00:01<00:24, 2937.06 examples/s]Map:   7%|â–‹         | 5000/76673 [00:01<00:27, 2627.00 examples/s]Map:   8%|â–Š         | 6000/76673 [00:02<00:25, 2794.43 examples/s]Map:   9%|â–‰         | 7000/76673 [00:02<00:27, 2560.34 examples/s]Map:  10%|â–ˆ         | 8000/76673 [00:02<00:25, 2651.50 examples/s]Map:  12%|â–ˆâ–        | 9000/76673 [00:03<00:24, 2785.31 examples/s]Map:  13%|â–ˆâ–Ž        | 10000/76673 [00:03<00:23, 2793.18 examples/s]Map:  14%|â–ˆâ–        | 11000/76673 [00:03<00:22, 2934.75 examples/s]Map:  16%|â–ˆâ–Œ        | 12000/76673 [00:04<00:22, 2881.59 examples/s]Map:  17%|â–ˆâ–‹        | 13000/76673 [00:04<00:22, 2838.46 examples/s]Map:  18%|â–ˆâ–Š        | 14000/76673 [00:05<00:22, 2847.00 examples/s]Map:  20%|â–ˆâ–‰        | 15000/76673 [00:05<00:21, 2926.79 examples/s]Map:  21%|â–ˆâ–ˆ        | 16000/76673 [00:05<00:23, 2559.18 examples/s]Map:  22%|â–ˆâ–ˆâ–       | 17000/76673 [00:06<00:25, 2341.30 examples/s]Map:  23%|â–ˆâ–ˆâ–Ž       | 18000/76673 [00:06<00:23, 2467.04 examples/s]Map:  25%|â–ˆâ–ˆâ–       | 19000/76673 [00:07<00:21, 2646.72 examples/s]Map:  26%|â–ˆâ–ˆâ–Œ       | 20000/76673 [00:07<00:20, 2759.69 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 21000/76673 [00:07<00:19, 2813.31 examples/s]Map:  29%|â–ˆâ–ˆâ–Š       | 22000/76673 [00:08<00:25, 2120.50 examples/s]Map:  30%|â–ˆâ–ˆâ–‰       | 23000/76673 [00:08<00:23, 2283.13 examples/s]Map:  31%|â–ˆâ–ˆâ–ˆâ–      | 24000/76673 [00:09<00:21, 2431.95 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 25000/76673 [00:09<00:20, 2574.46 examples/s]Map:  34%|â–ˆâ–ˆâ–ˆâ–      | 26000/76673 [00:09<00:18, 2677.93 examples/s]Map:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 27000/76673 [00:10<00:18, 2715.97 examples/s]Map:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 28000/76673 [00:10<00:17, 2755.01 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 29000/76673 [00:10<00:17, 2793.09 examples/s]Map:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 30000/76673 [00:11<00:16, 2797.08 examples/s]Map:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 31000/76673 [00:11<00:16, 2836.61 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 32000/76673 [00:11<00:15, 2839.87 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 33000/76673 [00:12<00:15, 2858.89 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 34000/76673 [00:12<00:15, 2799.79 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 35000/76673 [00:13<00:15, 2720.96 examples/s]Map:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 36000/76673 [00:13<00:14, 2762.59 examples/s]Map:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 37000/76673 [00:13<00:14, 2778.73 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 38000/76673 [00:14<00:14, 2696.49 examples/s]Map:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 39000/76673 [00:14<00:13, 2827.13 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 40000/76673 [00:14<00:12, 2944.39 examples/s]Map:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 41000/76673 [00:15<00:12, 2950.94 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 42000/76673 [00:15<00:11, 2929.13 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 43000/76673 [00:15<00:11, 2887.19 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 44000/76673 [00:16<00:10, 3005.04 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 45000/76673 [00:16<00:12, 2452.25 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 46000/76673 [00:16<00:11, 2658.92 examples/s]Map:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 47000/76673 [00:17<00:10, 2832.74 examples/s]Map:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 48000/76673 [00:17<00:09, 2948.57 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 49000/76673 [00:17<00:09, 3047.32 examples/s]Map:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 50000/76673 [00:18<00:09, 2700.58 examples/s]Map:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 51000/76673 [00:18<00:09, 2654.20 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 52000/76673 [00:19<00:08, 2777.34 examples/s]Map:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 53000/76673 [00:19<00:08, 2929.62 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 54000/76673 [00:19<00:07, 3018.40 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 55000/76673 [00:20<00:07, 2815.13 examples/s]Map:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 56000/76673 [00:20<00:07, 2850.02 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 57000/76673 [00:20<00:07, 2586.82 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 58000/76673 [00:21<00:07, 2660.71 examples/s]Map:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 59000/76673 [00:21<00:06, 2800.64 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 60000/76673 [00:21<00:05, 2813.09 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 61000/76673 [00:22<00:06, 2317.61 examples/s]Map:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 62000/76673 [00:22<00:05, 2545.81 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 63000/76673 [00:23<00:04, 2736.92 examples/s]Map:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 64000/76673 [00:23<00:05, 2524.63 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 65000/76673 [00:24<00:04, 2433.60 examples/s]Map:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 66000/76673 [00:24<00:04, 2559.27 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 67000/76673 [00:24<00:04, 2213.70 examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 68000/76673 [00:25<00:03, 2440.69 examples/s]Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 69000/76673 [00:25<00:02, 2599.69 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 70000/76673 [00:25<00:02, 2659.96 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 71000/76673 [00:26<00:02, 2728.42 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 72000/76673 [00:26<00:01, 2442.74 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 73000/76673 [00:27<00:01, 2542.51 examples/s]Map:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 74000/76673 [00:27<00:00, 2737.86 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 75000/76673 [00:27<00:00, 2863.57 examples/s]Map:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 76000/76673 [00:28<00:00, 2978.34 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76673/76673 [00:28<00:00, 2994.64 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76673/76673 [00:30<00:00, 2475.44 examples/s]
Map:   0%|          | 0/12030 [00:00<?, ? examples/s]Map:   8%|â–Š         | 1000/12030 [00:00<00:03, 2843.05 examples/s]Map:  17%|â–ˆâ–‹        | 2000/12030 [00:00<00:03, 3011.98 examples/s]Map:  25%|â–ˆâ–ˆâ–       | 3000/12030 [00:00<00:02, 3127.37 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4000/12030 [00:01<00:02, 3204.20 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5000/12030 [00:01<00:02, 2823.80 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 6000/12030 [00:02<00:02, 2824.29 examples/s]Map:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7000/12030 [00:02<00:01, 2963.21 examples/s]Map:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8000/12030 [00:02<00:01, 3091.38 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 9000/12030 [00:02<00:00, 3082.93 examples/s]Map:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10000/12030 [00:03<00:00, 2961.12 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11000/12030 [00:03<00:00, 2898.80 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 12000/12030 [00:04<00:00, 2998.67 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12030/12030 [00:04<00:00, 2645.25 examples/s]
Map:   0%|          | 0/10943 [00:00<?, ? examples/s]Map:   9%|â–‰         | 1000/10943 [00:00<00:03, 2769.65 examples/s]Map:  18%|â–ˆâ–Š        | 2000/10943 [00:00<00:03, 2834.71 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 3000/10943 [00:01<00:02, 2856.77 examples/s]Map:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 4000/10943 [00:01<00:02, 3007.99 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 5000/10943 [00:01<00:01, 3133.70 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 6000/10943 [00:02<00:02, 2433.17 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 7000/10943 [00:02<00:01, 2625.13 examples/s]Map:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 8000/10943 [00:03<00:01, 2490.08 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 9000/10943 [00:03<00:00, 2687.94 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 10000/10943 [00:03<00:00, 2845.92 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10943/10943 [00:03<00:00, 2971.45 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10943/10943 [00:04<00:00, 2499.92 examples/s]
/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/work/pi_jaimedavila_umass_edu/asharda_umass_edu/LLM_scripts/llama2_7b/train_bot.py:135: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Traceback (most recent call last):
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/LLM_scripts/llama2_7b/train_bot.py", line 135, in <module>
    trainer = Trainer(
              ^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/trainer.py", line 558, in __init__
    raise ValueError(
ValueError: You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details
