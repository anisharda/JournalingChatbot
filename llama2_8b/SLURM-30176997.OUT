Loading gmp version 6.2.1
Loading mpfr version 3.1.6
Loading mpc version 1.1.0
Loading zlib-ng version 2.1.6
Loading gcc version 9.4.0
2025-03-24 13:36:47.361829: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742823407.968746 3359318 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742823408.154452 3359318 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1742823409.809505 3359318 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1742823409.809575 3359318 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1742823409.809580 3359318 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1742823409.809598 3359318 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-03-24 13:36:49.950379: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [01:23<02:47, 83.81s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:42<01:20, 80.79s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:34<00:00, 67.59s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:34<00:00, 71.46s/it]
Starting the function!!
Map:   0%|          | 0/76673 [00:00<?, ? examples/s]Map:   1%|▏         | 1000/76673 [00:00<00:37, 2016.53 examples/s]Map:   3%|▎         | 2000/76673 [00:00<00:32, 2307.23 examples/s]Map:   4%|▍         | 3000/76673 [00:01<00:28, 2572.89 examples/s]Map:   5%|▌         | 4000/76673 [00:01<00:26, 2698.86 examples/s]Map:   7%|▋         | 5000/76673 [00:02<00:28, 2480.27 examples/s]Map:   8%|▊         | 6000/76673 [00:02<00:27, 2571.96 examples/s]Map:   9%|▉         | 7000/76673 [00:02<00:28, 2446.87 examples/s]Map:  10%|█         | 8000/76673 [00:03<00:27, 2454.97 examples/s]Map:  12%|█▏        | 9000/76673 [00:03<00:25, 2635.85 examples/s]Map:  13%|█▎        | 10000/76673 [00:03<00:25, 2654.11 examples/s]Map:  14%|█▍        | 11000/76673 [00:04<00:23, 2825.13 examples/s]Map:  16%|█▌        | 12000/76673 [00:04<00:22, 2891.30 examples/s]Map:  17%|█▋        | 13000/76673 [00:04<00:22, 2870.40 examples/s]Map:  18%|█▊        | 14000/76673 [00:05<00:23, 2684.88 examples/s]Map:  20%|█▉        | 15000/76673 [00:05<00:22, 2697.70 examples/s]Map:  21%|██        | 16000/76673 [00:06<00:24, 2427.40 examples/s]Map:  22%|██▏       | 17000/76673 [00:06<00:23, 2490.25 examples/s]Map:  23%|██▎       | 18000/76673 [00:06<00:22, 2615.33 examples/s]Map:  25%|██▍       | 19000/76673 [00:07<00:21, 2711.17 examples/s]Map:  26%|██▌       | 20000/76673 [00:07<00:20, 2742.77 examples/s]Map:  27%|██▋       | 21000/76673 [00:08<00:20, 2672.54 examples/s]Map:  29%|██▊       | 22000/76673 [00:08<00:22, 2438.00 examples/s]Map:  30%|██▉       | 23000/76673 [00:08<00:21, 2551.15 examples/s]Map:  31%|███▏      | 24000/76673 [00:09<00:20, 2521.58 examples/s]Map:  33%|███▎      | 25000/76673 [00:09<00:19, 2649.01 examples/s]Map:  34%|███▍      | 26000/76673 [00:09<00:18, 2742.32 examples/s]Map:  35%|███▌      | 27000/76673 [00:10<00:18, 2702.95 examples/s]Map:  37%|███▋      | 28000/76673 [00:10<00:17, 2755.95 examples/s]Map:  38%|███▊      | 29000/76673 [00:11<00:22, 2157.61 examples/s]Map:  39%|███▉      | 30000/76673 [00:11<00:20, 2318.59 examples/s]Map:  40%|████      | 31000/76673 [00:12<00:19, 2306.73 examples/s]Map:  42%|████▏     | 32000/76673 [00:12<00:18, 2422.16 examples/s]Map:  43%|████▎     | 33000/76673 [00:12<00:17, 2430.07 examples/s]Map:  44%|████▍     | 34000/76673 [00:13<00:16, 2513.46 examples/s]Map:  46%|████▌     | 35000/76673 [00:13<00:16, 2582.99 examples/s]Map:  47%|████▋     | 36000/76673 [00:14<00:15, 2545.78 examples/s]Map:  48%|████▊     | 37000/76673 [00:14<00:15, 2620.89 examples/s]Map:  50%|████▉     | 38000/76673 [00:14<00:15, 2514.65 examples/s]Map:  51%|█████     | 39000/76673 [00:15<00:15, 2465.29 examples/s]Map:  52%|█████▏    | 40000/76673 [00:15<00:14, 2540.63 examples/s]Map:  53%|█████▎    | 41000/76673 [00:15<00:13, 2656.51 examples/s]Map:  55%|█████▍    | 42000/76673 [00:16<00:12, 2723.84 examples/s]Map:  56%|█████▌    | 43000/76673 [00:16<00:12, 2749.94 examples/s]Map:  57%|█████▋    | 44000/76673 [00:17<00:12, 2682.80 examples/s]Map:  59%|█████▊    | 45000/76673 [00:17<00:11, 2648.50 examples/s]Map:  60%|█████▉    | 46000/76673 [00:17<00:11, 2691.85 examples/s]Map:  61%|██████▏   | 47000/76673 [00:18<00:11, 2636.71 examples/s]Map:  63%|██████▎   | 48000/76673 [00:18<00:11, 2602.79 examples/s]Map:  64%|██████▍   | 49000/76673 [00:18<00:10, 2632.56 examples/s]Map:  65%|██████▌   | 50000/76673 [00:19<00:12, 2176.37 examples/s]Map:  67%|██████▋   | 51000/76673 [00:20<00:11, 2254.70 examples/s]Map:  68%|██████▊   | 52000/76673 [00:20<00:11, 2231.43 examples/s]Map:  69%|██████▉   | 53000/76673 [00:20<00:10, 2334.08 examples/s]Map:  70%|███████   | 54000/76673 [00:21<00:12, 1853.63 examples/s]Map:  72%|███████▏  | 55000/76673 [00:22<00:11, 1920.61 examples/s]Map:  73%|███████▎  | 56000/76673 [00:22<00:10, 2064.13 examples/s]Map:  74%|███████▍  | 57000/76673 [00:23<00:09, 2084.66 examples/s]Map:  76%|███████▌  | 58000/76673 [00:23<00:08, 2255.42 examples/s]Map:  77%|███████▋  | 59000/76673 [00:23<00:07, 2475.72 examples/s]Map:  78%|███████▊  | 60000/76673 [00:24<00:06, 2583.71 examples/s]Map:  80%|███████▉  | 61000/76673 [00:24<00:07, 2150.95 examples/s]Map:  81%|████████  | 62000/76673 [00:25<00:06, 2333.52 examples/s]Map:  82%|████████▏ | 63000/76673 [00:25<00:05, 2427.32 examples/s]Map:  83%|████████▎ | 64000/76673 [00:25<00:05, 2321.38 examples/s]Map:  85%|████████▍ | 65000/76673 [00:26<00:05, 2263.17 examples/s]Map:  86%|████████▌ | 66000/76673 [00:26<00:04, 2276.20 examples/s]Map:  87%|████████▋ | 67000/76673 [00:27<00:04, 2397.68 examples/s]Map:  89%|████████▊ | 68000/76673 [00:27<00:03, 2398.53 examples/s]Map:  90%|████████▉ | 69000/76673 [00:27<00:03, 2418.14 examples/s]Map:  91%|█████████▏| 70000/76673 [00:28<00:02, 2487.08 examples/s]Map:  93%|█████████▎| 71000/76673 [00:28<00:02, 2498.79 examples/s]Map:  94%|█████████▍| 72000/76673 [00:29<00:02, 2167.60 examples/s]Map:  95%|█████████▌| 73000/76673 [00:29<00:01, 2247.27 examples/s]Map:  97%|█████████▋| 74000/76673 [00:30<00:01, 2347.07 examples/s]Map:  98%|█████████▊| 75000/76673 [00:30<00:00, 2419.66 examples/s]Map:  99%|█████████▉| 76000/76673 [00:30<00:00, 2497.20 examples/s]Map: 100%|██████████| 76673/76673 [00:31<00:00, 1912.18 examples/s]Map: 100%|██████████| 76673/76673 [00:34<00:00, 2219.63 examples/s]
Map:   0%|          | 0/12030 [00:00<?, ? examples/s]Map:   8%|▊         | 1000/12030 [00:00<00:04, 2559.23 examples/s]Map:  17%|█▋        | 2000/12030 [00:00<00:03, 2514.65 examples/s]Map:  25%|██▍       | 3000/12030 [00:01<00:03, 2518.42 examples/s]Map:  33%|███▎      | 4000/12030 [00:01<00:03, 2636.28 examples/s]Map:  42%|████▏     | 5000/12030 [00:01<00:02, 2502.91 examples/s]Map:  50%|████▉     | 6000/12030 [00:02<00:02, 2577.44 examples/s]Map:  58%|█████▊    | 7000/12030 [00:02<00:01, 2726.81 examples/s]Map:  67%|██████▋   | 8000/12030 [00:02<00:01, 2819.77 examples/s]Map:  75%|███████▍  | 9000/12030 [00:03<00:01, 2842.62 examples/s]Map:  83%|████████▎ | 10000/12030 [00:03<00:00, 2645.56 examples/s]Map:  91%|█████████▏| 11000/12030 [00:04<00:00, 2600.02 examples/s]Map: 100%|█████████▉| 12000/12030 [00:04<00:00, 2607.76 examples/s]Map: 100%|██████████| 12030/12030 [00:05<00:00, 2362.50 examples/s]
Map:   0%|          | 0/10943 [00:00<?, ? examples/s]Map:   9%|▉         | 1000/10943 [00:00<00:04, 2281.65 examples/s]Map:  18%|█▊        | 2000/10943 [00:00<00:03, 2429.03 examples/s]Map:  27%|██▋       | 3000/10943 [00:01<00:03, 2504.50 examples/s]Map:  37%|███▋      | 4000/10943 [00:01<00:02, 2724.54 examples/s]Map:  46%|████▌     | 5000/10943 [00:01<00:02, 2795.81 examples/s]Map:  55%|█████▍    | 6000/10943 [00:02<00:01, 2847.42 examples/s]Map:  64%|██████▍   | 7000/10943 [00:02<00:01, 2836.58 examples/s]Map:  73%|███████▎  | 8000/10943 [00:03<00:01, 2586.36 examples/s]Map:  82%|████████▏ | 9000/10943 [00:03<00:00, 1986.21 examples/s]Map:  91%|█████████▏| 10000/10943 [00:04<00:00, 2128.01 examples/s]Map: 100%|██████████| 10943/10943 [00:04<00:00, 2260.82 examples/s]Map: 100%|██████████| 10943/10943 [00:05<00:00, 2182.18 examples/s]
/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/work/pi_jaimedavila_umass_edu/asharda_umass_edu/LLM_scripts/llama2_7b/train_bot.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
GOT THROUGH TOKENIZIED DATA SETS
Traceback (most recent call last):
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/LLM_scripts/llama2_7b/train_bot.py", line 56, in <module>
    trainer = Trainer(
              ^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/trainer.py", line 612, in __init__
    self._move_model_to_device(model, args.device)
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/trainer.py", line 899, in _move_model_to_device
    model = model.to(device)
            ^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3162, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1343, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/torch/nn/modules/module.py", line 903, in _apply
    module._apply(fn)
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/torch/nn/modules/module.py", line 903, in _apply
    module._apply(fn)
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/torch/nn/modules/module.py", line 903, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/work/pi_jaimedavila_umass_edu/asharda_umass_edu/test_LLM/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1329, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 0 has a total capacity of 11.91 GiB of which 178.69 MiB is free. Including non-PyTorch memory, this process has 11.74 GiB memory in use. Of the allocated memory 11.64 GiB is allocated by PyTorch, and 1.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
